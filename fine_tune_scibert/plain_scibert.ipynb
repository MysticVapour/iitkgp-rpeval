{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>publishable</th>\n",
       "      <th>conference</th>\n",
       "      <th>sections</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3808_The_Distortion_of_Binomia</td>\n",
       "      <td>3808_The_Distortion_of_Binomia.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"The Distortion of Binomial Voting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461_LithoBench_Benchmarking_AI</td>\n",
       "      <td>461_LithoBench_Benchmarking_AI.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"LithoBench: Benchmarking AI Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9310_Multi_task_learning_with_</td>\n",
       "      <td>9310_Multi_task_learning_with_.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"Multi-Task Learning with Summary S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557_EmbodiedGPT_Vision_Languag</td>\n",
       "      <td>557_EmbodiedGPT_Vision_Languag.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"EmbodiedGPT: Vision-Language Pre-T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10107_Finite_Population_Regres</td>\n",
       "      <td>10107_Finite_Population_Regres.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"Finite Population Regression Adjus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id                           file_name  \\\n",
       "0  3808_The_Distortion_of_Binomia  3808_The_Distortion_of_Binomia.pdf   \n",
       "1  461_LithoBench_Benchmarking_AI  461_LithoBench_Benchmarking_AI.pdf   \n",
       "2  9310_Multi_task_learning_with_  9310_Multi_task_learning_with_.pdf   \n",
       "3  557_EmbodiedGPT_Vision_Languag  557_EmbodiedGPT_Vision_Languag.pdf   \n",
       "4  10107_Finite_Population_Regres  10107_Finite_Population_Regres.pdf   \n",
       "\n",
       "   publishable conference                                           sections  \n",
       "0            1    NeurIPS  {\"output\": \"The Distortion of Binomial Voting ...  \n",
       "1            1    NeurIPS  {\"output\": \"LithoBench: Benchmarking AI Comput...  \n",
       "2            1    NeurIPS  {\"output\": \"Multi-Task Learning with Summary S...  \n",
       "3            1    NeurIPS  {\"output\": \"EmbodiedGPT: Vision-Language Pre-T...  \n",
       "4            1    NeurIPS  {\"output\": \"Finite Population Regression Adjus...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to the SQLite database\n",
    "db_path = '../db/research_papers.db'  # Adjust to your actual path\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Load table into a Pandas DataFrame\n",
    "df = pd.read_sql_query(\"SELECT * FROM labelled_data\", conn)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>publishable</th>\n",
       "      <th>conference</th>\n",
       "      <th>sections</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3808_The_Distortion_of_Binomia</td>\n",
       "      <td>3808_The_Distortion_of_Binomia.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"The Distortion of Binomial Voting ...</td>\n",
       "      <td>The Distortion of Binomial Voting Defies Expec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>461_LithoBench_Benchmarking_AI</td>\n",
       "      <td>461_LithoBench_Benchmarking_AI.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"LithoBench: Benchmarking AI Comput...</td>\n",
       "      <td>LithoBench: Benchmarking AI Computational\\nLit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9310_Multi_task_learning_with_</td>\n",
       "      <td>9310_Multi_task_learning_with_.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"Multi-Task Learning with Summary S...</td>\n",
       "      <td>Multi-Task Learning with Summary Statistics\\nP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>557_EmbodiedGPT_Vision_Languag</td>\n",
       "      <td>557_EmbodiedGPT_Vision_Languag.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"EmbodiedGPT: Vision-Language Pre-T...</td>\n",
       "      <td>EmbodiedGPT: Vision-Language Pre-Training via\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10107_Finite_Population_Regres</td>\n",
       "      <td>10107_Finite_Population_Regres.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>NeurIPS</td>\n",
       "      <td>{\"output\": \"Finite Population Regression Adjus...</td>\n",
       "      <td>Finite Population Regression Adjustment and\\nN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               id                           file_name  \\\n",
       "0  3808_The_Distortion_of_Binomia  3808_The_Distortion_of_Binomia.pdf   \n",
       "1  461_LithoBench_Benchmarking_AI  461_LithoBench_Benchmarking_AI.pdf   \n",
       "2  9310_Multi_task_learning_with_  9310_Multi_task_learning_with_.pdf   \n",
       "3  557_EmbodiedGPT_Vision_Languag  557_EmbodiedGPT_Vision_Languag.pdf   \n",
       "4  10107_Finite_Population_Regres  10107_Finite_Population_Regres.pdf   \n",
       "\n",
       "   publishable conference                                           sections  \\\n",
       "0            1    NeurIPS  {\"output\": \"The Distortion of Binomial Voting ...   \n",
       "1            1    NeurIPS  {\"output\": \"LithoBench: Benchmarking AI Comput...   \n",
       "2            1    NeurIPS  {\"output\": \"Multi-Task Learning with Summary S...   \n",
       "3            1    NeurIPS  {\"output\": \"EmbodiedGPT: Vision-Language Pre-T...   \n",
       "4            1    NeurIPS  {\"output\": \"Finite Population Regression Adjus...   \n",
       "\n",
       "                                                text  \n",
       "0  The Distortion of Binomial Voting Defies Expec...  \n",
       "1  LithoBench: Benchmarking AI Computational\\nLit...  \n",
       "2  Multi-Task Learning with Summary Statistics\\nP...  \n",
       "3  EmbodiedGPT: Vision-Language Pre-Training via\\...  \n",
       "4  Finite Population Regression Adjustment and\\nN...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text_from_json(json_str):\n",
    "    \"\"\"Extract the 'output' field from the JSON string.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "        return data.get('output', '')\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return ''\n",
    "\n",
    "# Apply extraction\n",
    "df['text'] = df['sections'].apply(extract_text_from_json)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and model\n",
    "model_name = \"allenai/scibert_scivocab_uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# For binary classification, we specify num_labels=2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, tokenizer, max_tokens=512, overlap=50):\n",
    "    \"\"\"\n",
    "    Splits the text into overlapping chunks of up to max_tokens (tokenized).\n",
    "    Returns a list of raw text chunks.\n",
    "    \"\"\"\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # We will store the text chunks here\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = start + max_tokens\n",
    "        # Slice the token list to get the chunk\n",
    "        chunk_tokens = tokens[start:end]\n",
    "        # Convert back to text\n",
    "        chunk_text = tokenizer.convert_tokens_to_string(chunk_tokens)\n",
    "        chunks.append(chunk_text)\n",
    "        # Move the start index by (max_tokens - overlap)\n",
    "        start += (max_tokens - overlap)\n",
    "        \n",
    "        if start >= len(tokens):\n",
    "            break\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_with_aggregation(text, tokenizer, model, max_tokens=512, overlap=120, aggregation='mean'):\n",
    "    \"\"\"\n",
    "    Classify a text by splitting into chunks and aggregating the logits.\n",
    "    aggregation: 'mean' or 'max'.\n",
    "    Returns predicted label (0 or 1).\n",
    "    \"\"\"\n",
    "    # Get all chunks\n",
    "    chunks = chunk_text(text, tokenizer, max_tokens, overlap)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    all_logits = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for chunk in chunks:\n",
    "            # Prepare model inputs\n",
    "            inputs = tokenizer(\n",
    "                chunk, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True,  # Truncate inputs longer than the max model length\n",
    "                max_length=max_tokens,  # Ensure the length matches max_tokens\n",
    "                padding=\"max_length\"   # Add padding to meet max_tokens length\n",
    "            )\n",
    "            # Move inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits  # shape: [batch_size, num_labels], here batch_size=1\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "\n",
    "    # all_logits is a list of numpy arrays, each of shape (1, 2)\n",
    "    all_logits = np.vstack(all_logits)  # shape => (#chunks, 2)\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        agg_logits = np.mean(all_logits, axis=0)  # shape => (2, )\n",
    "    elif aggregation == 'max':\n",
    "        agg_logits = np.max(all_logits, axis=0)\n",
    "    else:\n",
    "        raise ValueError(\"Aggregation must be 'mean' or 'max'.\")\n",
    "\n",
    "    # Predicted label is the index of the max logit\n",
    "    predicted_label = int(np.argmax(agg_logits))\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for document at index 0: 1\n"
     ]
    }
   ],
   "source": [
    "# Example usage on a sample document\n",
    "sample_index = 0  # you can pick any valid index\n",
    "sample_text = df.loc[sample_index, 'text']\n",
    "\n",
    "pred_label = classify_with_aggregation(sample_text, tokenizer, model, max_tokens=512, overlap=50, aggregation='mean')\n",
    "print(f\"Predicted label for document at index {sample_index}: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping index 46 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Skipping index 60 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Skipping index 79 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Skipping index 104 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Skipping index 109 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "Skipping index 136 due to error: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "skipped_indices = []  # To track rows that caused errors\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text_data = row['text']\n",
    "    try:\n",
    "        # Skip rows with empty or invalid text\n",
    "        if not text_data or not isinstance(text_data, str) or text_data.strip() == \"\":\n",
    "            raise ValueError(\"Invalid or empty text\")\n",
    "        \n",
    "        # Classify the text\n",
    "        label = classify_with_aggregation(text_data, tokenizer, model, max_tokens=512, overlap=50, aggregation='mean')\n",
    "        predictions.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping index {idx} due to error: {e}\")\n",
    "        predictions.append(None)  # Use None or a default value\n",
    "        skipped_indices.append(idx)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "df['predicted_label'] = predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "Precision: 0.85\n",
      "Recall: 1.00\n",
      "F1 Score: 0.92\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0  44]\n",
      " [  0 257]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        44\n",
      "           1       0.85      1.00      0.92       257\n",
      "\n",
      "    accuracy                           0.85       301\n",
      "   macro avg       0.43      0.50      0.46       301\n",
      "weighted avg       0.73      0.85      0.79       301\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/advay/Projects/iitkgp-rpeval/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/advay/Projects/iitkgp-rpeval/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/advay/Projects/iitkgp-rpeval/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Drop rows where predictions are None to ensure valid comparisons\n",
    "valid_df = df.dropna(subset=['predicted_label'])\n",
    "\n",
    "# Ground truth and predictions\n",
    "y_true = valid_df['publishable'].astype(int)  # Ensure integer type\n",
    "y_pred = valid_df['predicted_label'].astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# Display results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
